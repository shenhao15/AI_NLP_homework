{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltp\n",
    "from gensim.models import Word2Vec\n",
    "from pyltp import Segmentor\n",
    "import jieba\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from pyltp import  SentenceSplitter,NamedEntityRecognizer,Postagger,Parser,Segmentor\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = '/home/kg/PycharmProjects/TextGrapher-master/ltp_data/sqlResult_1558435.csv'\n",
    "data = pd.read_csv(data_source,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = data['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+',string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.920 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "news_content = [token(n) for n in content]\n",
    "news_content = [''.join(n) for n in news_content]\n",
    "\n",
    "news_content = [cut(n) for n in news_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = ('n', 'nr', 'ns', 'nt', 'eng', 'v', 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('/home/kg/PycharmProjects/TextGrapher-master/ltp_data/百度停用词表.txt',encoding='utf-8') as f:\n",
    "    for word in f.readlines():\n",
    "        stop_words.append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as jp,jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content=news_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ls = []\n",
    "for text in news_content:\n",
    "    words = [w.word for w in jp.cut(text) if w.flag in flags and w.word not in stop_words]\n",
    "    words_ls.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model = gensim.models.Word2Vec(words_ls, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(ini_words, model=news_model, max_len=7):\n",
    "    unseen = ini_words\n",
    "    seen = defaultdict(int)\n",
    "    \n",
    "    while unseen and len(seen)<max_len:\n",
    "        word = unseen.pop(0)\n",
    "        \n",
    "        similar_words = [w for w,_ in model.most_similar(word, topn=5)]\n",
    "        \n",
    "        unseen += similar_words\n",
    "        \n",
    "        seen[word] += 1\n",
    "        \n",
    "    return sorted(seen, key=lambda x:seen[x], reverse=True)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['说', '还', '人', '会', '都', '记者', '中国']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "wl = ['说']\n",
    "similar_say = [w for w in get_similar_words(wl) if w not in['了解','立场','估算','测算','统计','不禁','不已','一句','看来']]\n",
    "print(similar_say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_similar_words 效果不太好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_model = \"/home/kg/PycharmProjects/TextGrapher-master/ltp_data/ltp_data_v3.4.0/cws.model\"\n",
    "pos_model = \"/home/kg/PycharmProjects/TextGrapher-master/ltp_data/ltp_data_v3.4.0/pos.model\"\n",
    "par_model = \"/home/kg/PycharmProjects/TextGrapher-master/ltp_data/ltp_data_v3.4.0/parser.model\"\n",
    "ner_model = \"/home/kg/PycharmProjects/TextGrapher-master/ltp_data/ltp_data_v3.4.0/ner.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(u'\\r\\n', u'')\n",
    "    text = text.replace(u'\\u3000', u'')\n",
    "    text = text.replace(u'\\n', u'')\n",
    "    text = text.replace(u'\\\\r\\\\n', u'')\n",
    "    text = text.replace(u'\\\\u3000', u'')\n",
    "    text = text.replace(u'\\\\n', u'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    #得到分句子\n",
    "    st = ''\n",
    "    sents = SentenceSplitter.split(text)\n",
    "    sentences = [s for s in sents if len(s) != 0]\n",
    "    sentences = [preprocess(s) for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def get_word_list(sentence,model=cws_model):\n",
    "    #得到分词\n",
    "    segmentor = Segmentor()\n",
    "    segmentor.load(model)\n",
    "    word_list = list(segmentor.segment(sentence))\n",
    "    segmentor.release()\n",
    "    return word_list\n",
    "\n",
    "def get_postag_list(word_list,model=pos_model):\n",
    "    #得到词性标注\n",
    "    postag = Postagger()\n",
    "    postag.load(model)\n",
    "    postag_list = list(postag.postag(word_list))\n",
    "    postag.release()\n",
    "    return postag_list\n",
    "\n",
    "def get_parser_list(word_list,postag_list,model=par_model):\n",
    "    #得到依存关系\n",
    "    parser = Parser()\n",
    "    parser.load(model)\n",
    "    arcs = parser.parse(word_list,postag_list)\n",
    "    arc_list = [(arc.head,arc.relation) for arc in arcs]\n",
    "    parser.release()\n",
    "    return arc_list\n",
    "\n",
    "def get_ner_list(word_list,postag_list,model=ner_model):\n",
    "    #命名实体识别\n",
    "    reg = NamedEntityRecognizer()                    #初始化命名实体实例\n",
    "    reg.load(model)                                       #加载模型\n",
    "    netags = reg.recognize(word_list, postag_list)         #对分词、词性标注得到的数据进行实体标识\n",
    "    netags = list(netags)\n",
    "    reg.release()\n",
    "    return netags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(text):\n",
    "    ner_set = ['S-Ni', 'S-Ns', 'S-Nh', 'B-Ni', 'B-Ns', 'B-Nh', 'I-Ni', 'I-Ns', 'I-Nh', 'E-Ni', 'E-Ns', 'E-Nh']\n",
    "    punc_set = ['，',',','：',';','；','。','!','！','？','?']\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "    words = [get_word_list(sent) for sent in sentences]\n",
    "    postags = [get_postag_list(w) for w in words]\n",
    "    parsers = [get_parser_list(w,p) for w, p in zip(words, postags)]\n",
    "    ners = [get_ner_list(w,p) for w, p in zip(words, postags)]\n",
    "    \n",
    "    #取‘说’的近义词\n",
    "    similar_say = ['表示', '说', '指出', '认为', '坦言', '告诉', '强调', '称', '直言', '普遍认为', '介绍', '透露', '重申', '呼吁', '说道', '感叹', '地说', '写道', '中称', '证实', '还称', '猜测', '暗示', '感慨', '热议', '敦促', '指责', '声称', '主张', '反对', '批评', '表态', '中说', '承认', '却说', '感触', '提到', '所说', '引述', '质疑', '抨击']\n",
    "    similar_say += ['回应','分析']\n",
    "    content = {}\n",
    "    start = 0\n",
    "    end = -1\n",
    "    said = []\n",
    "    for i in range(len(sentences)):\n",
    "        if not sentences[i]:\n",
    "            continue\n",
    "        for j in range(len(words[i])):\n",
    "            #主谓关系找主语和谓语为‘说’相近的动词的句子，这里只考虑主语为特定实体（人名、地名、机构名）或者一般性名词，\n",
    "            if parsers[i][j][1]=='SBV' and (ners[i][j]!='O' or postags[i][j]=='n'):\n",
    "                pos = parsers[i][j][0]-1\n",
    "\n",
    "                #谓语动词为‘说’近义词或者谓语动词下一个词与谓语动词为并列关系且为‘说’的同义词\n",
    "                if words[i][pos] in similar_say or '说' in words[i][pos] or (parsers[i][pos+1][1]=='COO' and parsers[i][pos+1][0]==(pos+1) and words[i][pos+1] in similar_say): \n",
    "                                    \n",
    "                    #之前句子内的言论      \n",
    "                    if ('”' in words[i-1]) and ('说' not in sentences[i-1]) and i>0:\n",
    "                        m = words[i-1].index('”')\n",
    "                        if ('“' in words[i-1]):\n",
    "                            n = words[i-1].index('“')\n",
    "                            if m-n >3:\n",
    "                                said = said+words[i-1][n+1:m]\n",
    "                        else:\n",
    "                            for k in range(1,i-1):\n",
    "                                if '“' in words[i-1-k]:\n",
    "                                    n = words[i-1-k].index('“')\n",
    "                                    said = said+words[i-1-k][n+1:]+words[i-1][:m]\n",
    "                                    break\n",
    "                    \n",
    "                    #本句前面的言论\n",
    "                    if ('”' in words[i][:pos]):\n",
    "                        m = words[i].index('”')\n",
    "                        if ('“' in words[i]):\n",
    "                            n = words[i].index('“')\n",
    "                            said = said+words[i][n+1:m]\n",
    "                        else:\n",
    "                            for k in range(1,i):\n",
    "                                if '“' in words[i-k]:\n",
    "                                    n = words[i-k].index('“')\n",
    "                                    said = said+words[i-k][n+1:]+words[i][:m]\n",
    "                                    break\n",
    "                                    \n",
    "                    #本句后面的言论\n",
    "                    start = pos+1\n",
    "                    if words[i][start] in punc_set:\n",
    "                        start += 1\n",
    "                    said = said+words[i][start:]\n",
    "                      \n",
    "                    #之后句子内的言论\n",
    "                    if ('“' in words[i+1]) and ('说' not in sentences[i+1]) and i<len(sentences)-1:\n",
    "                        m = words[i+1].index('“')\n",
    "                        if ('”' in words[i+1]):\n",
    "                            n = words[i+1].index('”')\n",
    "                            if n-m >3: \n",
    "                                said = said+words[i+1][m+1:n]\n",
    "                        else:\n",
    "                            for k in range(i+2,len(sentences)):\n",
    "                                if '”' in words[k]:\n",
    "                                    n = words[k].index('”')\n",
    "                                    said = said+words[i+1][m+1:] + words[k][:n]\n",
    "                                    break\n",
    "\n",
    "                    if ners[i][j]!='O' and postags[i][j]=='n':\n",
    "                        name = words[i][j-1:j+1]\n",
    "                    else:\n",
    "                        name = words[i][j]\n",
    "                    \n",
    "                    if name in content.keys():# 同一人/机构/团体等的言论合并\n",
    "                        content[name] += [''.join(said)]\n",
    "                    else:\n",
    "                        content[name] = [''.join(said)]\n",
    "                        \n",
    "                    said = []\n",
    "                    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u3000北京时间6月20日，据sportando报道，有CBA球队与上赛季效力于西班牙联赛的法国后卫埃德温-杰克逊联系到了一起，不过目前尚不清楚具体是哪家俱乐部。\\r\\n\\u3000\\u3000杰克逊的经纪人今天在推特上用中文写下了埃德温-杰克逊的名字，这被认为是在暗示该球员未来的去向。杰克逊也似乎在推特上向他的球迷告别，同时还表态称，虽然自己更愿意留在欧洲联赛或者去NBA发展，但他觉得这份合同（来CBA打球）仅仅是一份6个月的短合同，之后他还会有其他选择。\\r\\n\\u3000\\u3000埃德温-杰克逊生于1989年9月18日，身高190cm的他在场上司职得分后卫。上赛季埃德温-杰克逊在西甲联赛的大学生队效力期间，场均出场30.3分钟能够拿到21.4分，3.5个篮板，3.4次助攻以及1次抢断的数据。\\r\\n\\u3000\\u3000据欧洲篮球专家王健微博透露，这名法国后卫有超强的个人得分能力，堪称刷分机器。其个人能力与美籍后卫相比，也丝毫不逊色。\\r\\n\\u3000\\u3000（豪斯）\\r\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'微博': ['这名法国后卫有超强的个人得分能力，堪称刷分机器。']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_speech(content[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '\\\\n\\\\n\\\\n新华社北京5月8日新媒体专电;英媒称，斯蒂芬·霍金教授警告说，人类如果想要生存下去，就必须在100年内逃离地球。\\\\n\\\\n\\\\n\\\\n据英国《每日邮报》网站5月3日报道，这位著名理论物理学家认为，地球上的生命被灾难毁灭的风险越来越大，包括小行星撞击、传染病、人口过多和气候变化等。\\\\n\\\\n\\\\n霍金教授在一部新纪录片中警告说，在今后一个世纪里，我们的世界会变得越来越不宜居，未来几代人必须在太空中寻找新的生存空间。\\\\n\\\\n\\\\n报道称，在新纪录片《远征新地球》中，霍金教授将周游世界，探讨人类如何在外太空生存。这部纪录片是英国广播公司（BBC）将于6月开播的《明日世界》节目的一部分。\\\\n\\\\n\\\\n在这部将由BBC二台播出的纪录片中，霍金声称人类能够在地球上生存的时间所剩无几，我们必须去其他地方探寻自己的未来。\\\\n\\\\n\\\\n报道称，去年11月，霍金对于自己的估计还比较保守。他当时曾警告说，人类无法在“脆弱”的地球上再生存1000年。\\\\n\\\\n\\\\n在那个谈话节目中，霍金用1小时的时间快速回顾了人类对宇宙起源的认识的发展历史，从最初的创世神话一直谈到了最先进的M理论预言。\\\\n\\\\n\\\\n他说：“也许有朝一日我们能够利用引力波来回看宇宙大爆炸的中心。”\\\\n\\\\n\\\\n“宇宙学的最新进展都是在太空中取得的，在那里可以不受影响地观测我们的宇宙，但我们继续探索太空也是为了人类的未来，”他说，“我认为，如果不逃离我们脆弱的星球，我们将无法再生存1000年。”\\\\n\\\\n\\\\n霍金接着说：“因此我希望激发公众对太空的兴趣，而我也一直在进行早期训练。”他曾说自己希望搭乘维珍集团老板理查德·布兰森的维珍银河飞船前往太空。\\\\n\\\\n\\\\n霍金还说：“能够活着并从事理论物理学研究，这真是一段美好的时光。在过去50年里，我们对宇宙的认识发生了巨大变化。如果我为此作出了一份小小贡献的话，那么我感到十分高兴。”\\\\n\\\\n\\\\n他说：“我们人类只是自然界基本粒子的组合，而我们能够如此接近于理解那些支配我们和宇宙的规律，这本身就是一项伟大的成就。”\\\\n\\\\n\\\\n报道称，霍金以前曾在《如何制造宇宙飞船》一书的编后语中谈过他对未来太空旅行的观点。\\\\n\\\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'报道': ['在新纪录片《远征新地球》中，霍金教授将周游世界，探讨人类如何在外太空生存。',\n",
       "  '去年11月，霍金对于自己的估计还比较保守。',\n",
       "  '霍金以前曾在《如何制造宇宙飞船》一书的编后语中谈过他对未来太空旅行的观点。'],\n",
       " '教授': ['说，人类如果想要生存下去，就必须在100年内逃离地球。',\n",
       "  '说，在今后一个世纪里，我们的世界会变得越来越不宜居，未来几代人必须在太空中寻找新的生存空间。'],\n",
       " '物理学家': ['地球上的生命被灾难毁灭的风险越来越大，包括小行星撞击、传染病、人口过多和气候变化等。'],\n",
       " '霍金': ['人类能够在地球上生存的时间所剩无几，我们必须去其他地方探寻自己的未来。',\n",
       "  '“因此我希望激发公众对太空的兴趣，而我也一直在进行早期训练。”',\n",
       "  '“能够活着并从事理论物理学研究，这真是一段美好的时光。']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_speech(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
