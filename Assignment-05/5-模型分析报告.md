# 一、自动摘要问题简介
自动摘要问题是NLP领域的一个经典问题，简单的说，就是输入一段长文字，输出对这段长文字的一个总结概要。 在新闻，语音播报、文档信息提取、公司报表、上市公司分析等等领域具有很多的应用场景。
# 二、算法原理
语料库 =》词向量 =》 句向量 =》 句子与全文相关度（SIF加权模型） =》 KNN平滑（对句子与全文相关度进行加权求和）=》排序 =》输出
# 三、算法实现
语料库处理：      
    news_processing.py      
    wiki_processing.py      

生成词向量：      
    wordembedding.py

提取摘要：
textsummary.py
```
# -*- coding: utf-8 -*-

from gensim.models import KeyedVectors
import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr
import pandas as pd
# from nltk.tokenize import StanfordTokenizer
import networkx as nx
import jieba
import re

path='/home/student/nlp_group_stem/nlp_p1/web/model/'
wv_model=KeyedVectors.load_word2vec_format(path + 'model_skipgram.model',binary=False)
word_dict=np.load(path+'words_dict.npy',allow_pickle=True)
embedding_size = 200#词向量向量大小

# 定义Word类，Word.text为词语，Word.vector为该词语的词向量
class Word:
    def __init__(self, text, vector):
        self.text = text
        self.vector = vector

# 定义Sentence类，Sentence.word_list为包含一串Word类的列表，Sentence.length为该列表的长度
class Sentence:
    def __init__(self, word_list):
        self.word_list = word_list

    def length(self):
        return len(self.word_list)

#定义word_frequency函数，函数返回word_text在wordfreq_dict中出现的频率
def word_frequency(word_text, wordfreq_dict):
    if word_text in wordfreq_dict:
        return wordfreq_dict[word_text]
    else:
        return 1.0#此处应该为0吧？

#定义sentence2vec函数，生成句向量
#SIF 加权模型
#    SIF 的计算分为两步：
#    1） 对句子中的每个词向量，乘以一个权重 a/(a+p_w)，其中 a 是一个常数（原文取 0.0001），p_w 为该词的词频；对于出现频率越高的词，其权重越小；
#    2） 计算句向量矩阵的第一个主成分u，让每个句向量减去它在u上的投影（类似 PCA）；
def sentence2vec(sentence_list, embedding_size, wordfreq_dict, a=1e-3):
    sentence_processed = []
    for sentence in sentence_list:
        vecsum = np.zeros(embedding_size)
        len_sentence = sentence.length()
        for word in sentence.word_list:
            a_coef = a / (a + word_frequency(word.text, wordfreq_dict))
            vecsum = np.add(vecsum, np.multiply(a_coef, word.vector))
        vecsum = np.divide(vecsum, len_sentence)
        sentence_processed.append(vecsum)
    pram_components=min(len(sentence_list), 20)
    pca = PCA(n_components=pram_components)
    pca.fit(np.array(sentence_processed))
    u = pca.components_[0]
    u = np.multiply(u, np.transpose(u))
    if len(u) < embedding_size:
        for i in range(embedding_size - len(u)):
            u = np.append(u, 0)
    sentence_cut = []
    for vecsum in sentence_processed:
        sentence_cut.append(vecsum - np.multiply(u, vecsum))
    return sentence_cut

#定义句get_title_vector函数，获取标题的句向量
def get_title_vector(title):
    sentence_vec = []
    sentences = []
    sentence = jieba.cut(title)
    for word in sentence:
        try:
            vec = wv_model[word]
        except KeyError:
            vec = np.zeros(embedding_size)
        sentence_vec.append(Word(word, vec))
    sentence_unit = Sentence(sentence_vec)
    sentences.append(sentence_unit)
    sentence_vectors = sentence2vec(sentences, embedding_size, word_dict)
    return sentence_vectors

#相邻元素求平均值
def knn_smooth_nearby(scores):
    knn_scores=[]
    n=len(scores)
    if((n-2)>0):
        knn_scores.append((scores[0]+scores[1])/2)
        for i in range(1,n-1):
            knn_scores.append((scores[i-1]+scores[i]+scores[i+1])/3)
        knn_scores.append((scores[n-1]+scores[n-2])/2)
    else:
        knn_scores=scores
    return knn_scores

def model(X, y):
    return [(Xi, yi) for Xi, yi in zip(X, y)]

def predict(x, s_set, k=3):
    most_similars = sorted(model(s_set, s_set), key=lambda xi: abs(xi[0]-x))[:k]
    return np.mean([i[1] for i in most_similars])

#KNN平滑函数
def knn_Smooth(scores,k=3):
    knn_scores=[]
    for s in scores:
        t=predict(s,scores,k)
        print("t",t)
        knn_scores.append(t)
    return knn_scores

#定义get_summary函数，获取摘要
def get_summary(input_title,input_body,rate=0.3):
    summary=""                      #summary默认为空
    title = input_title             #标题title为input_title
    article = input_body            #文章为input_body
    rate=float(rate)                #把rate转换为浮点型
    if pd.isnull(article):          #如果文章内容为空
        if pd.notnull(title):       #如果标题不为空
            summary = title         #摘要=标题
        else:                       #否则
            summary = 'no information'  #摘要=没有信息
    else:
        article = re.split('！|。|？', article.replace('\r\r\n', ''))    #对文章内容进行处理，用。号剥离出句子，并去除换行符
        sentences = []                                                  #定义sentences为列表
        raw_sentences = []                                              #定义raw_sentences为列表
        for sentence in article:
            if sentence != '':                                          #如果sentence不为空
                sentence_vec = []
                raw_sentence = sentence.replace(' ', '') + '。'         #raw_sentence等于sentence去除空格，并加上。
                if raw_sentence not in raw_sentences:                   #如果raw_sentence不在raw_sentences里
                    raw_sentences.append(raw_sentence)                  #追加raw_sentence到raw_sentences列表里
                    sentence = jieba.cut(sentence)                      #对sentence进行分词
                    for word in sentence:                               #对于sentence里的每一个word（词语）
                        try:
                            vec = wv_model[word]                        #尝试从wv_model模型中提取word的词向量
                        except KeyError:
                            vec = np.zeros(embedding_size)              #如果，词向量模型里没有word，生成一个全为0的词向量
                        sentence_vec.append(Word(word, vec))            #在sentence_vec里追加Word（包括词语和该词语词向量的类，上面有定义），sentence_vec会在sentence循环中重置
                    sentence_unit = Sentence(sentence_vec)              #sentence_unit为Sentence类，赋值为Sentence(sentence_vec)，内容包括，一个词向量列表和其长度
                    if(sentence_unit.length()>0):                       #如果sentence_unit的长度大于0
                        sentences.append(sentence_unit)                 #将sentence_unit追加至sentences列表里
        # 利用TIF求出句子向量集合-sentence_vectors，全文向量-paragragh_vector，标题向量-title_vectors
        sentence_vectors = sentence2vec(sentences, embedding_size, word_dict)   #调用sentence2vec函数，获取句向量
        paragragh_vector=np.mean(sentence_vectors,axis=0)                       #调用np.mean,对句向量的各列求均值，返回全文向量(跟句向量一样的结构)
        title_vectors=get_title_vector(title)                                   #调用get_title_vector函数，获取标题向量

        #求出每个句子向量与标题向量、全文向量的相似度，相加得到每个句子的相似度得分。
        scores=[]
        for sv in sentence_vectors:
            score=cosine_similarity(sv.reshape(1, embedding_size),paragragh_vector.reshape(1, embedding_size))
            +cosine_similarity(sv.reshape(1, embedding_size),title_vectors[0].reshape(1, embedding_size))
            scores.append(score)
        scores[0] += 0.2
        scores[len(scores)-1] += 0.2
        # 利用KNN对句子相似度得分进行平滑
        # knn_scores=knn_Smooth(scores,3)
        # 相邻三个元素求平均值
        knn_scores = knn_smooth_nearby(scores)
        # 整理求出topn个句子
        ranked_sentences = sorted(((knn_scores[i], s) for i, s in enumerate(raw_sentences)), reverse=True)
        top_sentense = {}
        p_num = max(int(len(ranked_sentences) * rate), 2)
        for top_num in range(min(len(ranked_sentences), p_num)):
            for sentence in article:
                if (sentence == (ranked_sentences[top_num][1]).replace('。', '')):
                    top_sentense[article.index(sentence)]=ranked_sentences[top_num][1]
        sort_sentense=dict(sorted(top_sentense.items(), key=lambda item:item[0]))
        for s in sort_sentense.values():
            summary+=s
        print("result:",summary)
    return summary                                                                          #返回summary，得到摘要
```

# 四、模型优缺点
##### 优点：
* 1、可以通过图形化的方式，摘取文章中的主要句子作为摘要   
* 2、SIF 加权模型获取句向量，相对与RNN、CNN等基于神经网络训练的方式来说，更容易理解。   

##### 缺点：
* 1、以句子作为摘要，使得摘要过于臃肿；   
* 2、对特殊类型的句子没有判断能力；如对于存在序号的，像“一、……“、“二、……”这种句子不能有效剔除序号，出现在摘要里显得不伦不类。