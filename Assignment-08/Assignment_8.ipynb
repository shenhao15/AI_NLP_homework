{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Autoencoder是一种无监督学习过程，由encode和decode构成，给定输入之后，经过encode将输入编码成code，然后在经过decode将code解码成输出，通过不断地训练，使得输入和输出尽可能相似。\n",
    "    通过控制encode的输出维数，可以实现以低维参数学习高维特征，实现了降维。在训练的过程中，主要使用反向传播进行优化，使得输入和输出尽可能相似。\n",
    "    encode和decode两个过程可以理解成互为反函数，在encode过程不断降维，在decode过程提高维度。\n",
    "    当AutoEncoder过程中用卷积操作提取特征，相当于encode过程为一个深度卷积神经网络，好多层的卷积池化，那么decode过程就需要进行反卷积和反池化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greddy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:贪心搜索（greddy search） 和 集束搜索（beam search）是用来解决seq2seq模型的解码问题。贪心搜索直接选择每个输出的最大概率，直到出现终结符或最大句子长度；集束搜索选择概率最大的前K个，集束搜索本质也是贪心思想，只不过它考虑了更多的候选搜索空间，可以得到更多的结果。集束搜索属于贪心算法，但不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。贪心搜索可以认为beam size为1时的集束搜索特例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:深度学习中的注意力机制（Attention Mechanism）和人类视觉的注意力机制类似，就是在众多信息中把注意力集中放在重要的点上，选出关键信息，而忽略其他不重要的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:不能解决一词多义问题；Word Embedding无法解决多义词的问题，同一个词在不同的上下文中表示不同的意思，但是在Word Embedding中一个词只有一个表示，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分。在实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本质是根据当前上下文对Word Embedding进行动态调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:与基于 RNN 的方法相比，Transformer不需要循环，而是并行处理序列中的所有单词或符号，同时利用自注意力机制将上下文与较远的单词结合起来。通过并行处理所有单词，并让每个单词在多个处理步骤中注意到句子中的其他单词，Transformer的训练速度比 RNN 快很多，而且其翻译结果也比RNN好得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:BN针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差；\n",
    "    LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；\n",
    "    LN不依赖于batch的大小和输入sequence的深度，因此可以用于Transformer中输入的normalize操作。\n",
    "   BN是沿batch size方向做归一化，这在NLP问题的Transformer模型中是没有意义的；LN是沿词向量方向做归一化，是合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:在RNN中，对句子的处理是一个个word按顺序输入的。但在Transformer中，输入句子的所有word是同时处理的，没有考虑词的排序和位置信息。因此，提出了positional encoding的方法来解决这个问题。positional encoding使得 Transformer 可以衡量 word 位置有关的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Self-Attention是Attention的特殊形式，是在序列内部做Attention，寻找序列内部的联系；\n",
    "    例如输入一个句子，那么里面的每个词都要和该句子中的所有词进行attention计算，目的是学习句子内部的词依赖关系，捕获句子的内部结构。\n",
    "    Attention(q_{t}, K, V)，也就是输入句中的某个文字，再将所有输入句中的文字依次用矩阵Attention(Q,K,V)来解决。\n",
    "    \n",
    "   multi-head attention，把key, value, query线性投射到不同空间h次，分别变成维度d_{q},d_{k},d_{v}，再各自做attention，其中，d_{k}=d_{v}=d_{model}/h=64，就是投射到h个head上。\n",
    "   Multi-head Self-Attention可以理解为考虑多种语义场景下目标字与文本中其它字的语义向量的不同融合方式。Multi-head Self-Attention的输入和输出在形式上完全相同，输入为文本中各个字的原始向量表示，输出为各个字融合了全文语义信息后的增强向量表示。因此，Multi-head Self-Attention可以看作是对文本中每个字分别增强其语义向量表示的黑盒。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:12-layer decoder，transformer decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:GPT是通过预训练学习和有针对性的微调的一种强有力的模型框架，通过预训练不同的长文本连续数据集，模型有能力处理长而广的依赖关系，这个是解决问答系统、语义相似度、文本分类中的关键。\n",
    "GPT 1.0采取预训练+FineTuning两个阶段，它采取Transformer作为特征抽取器。预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号来代替它们。尽管模型最终还是会看到所有位置上的输入信息，但由于需要预测的词已经被特殊符号代替，所以模型无法事先知道这些位置上是什么词，这样就可以让模型根据所给的标签去学习这些地方该填的词了。\n",
    "给定一句话，随机抹去这句话中的一个或几个词，要求根据剩余词汇预测被抹去的几个词分别是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "模型输入包括：\n",
    "1.文本中各个字/词的原始词向量；\n",
    "2.文本向量：该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/词的语义信息相融合；\n",
    "3.位置向量：由于出现在文本不同位置的字/词所携带的语义信息存在差异;\n",
    "最后，BERT模型将原始字词向量、文本向量和位置向量加和作为模型输入。\n",
    "模型输出：模型输出则是输入各字对应的融合全文语义信息后的向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1. 单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类。可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。\n",
    "2. 语句对分类任务：该任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分。\n",
    "3. 序列标注任务：该任务的实际应用场景包括：中文分词&新词发现（标注每个字是词的首字、中间字或末字）、答案抽取（答案的起止位置）等。对于该任务，BERT模型利用文本中每个字对应的输出向量对该字进行标注（分类）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "GPT1.0是单向语言模型；\n",
    "Bert是双向语言模型任务；\n",
    "GPT2.0仍然是单向语言模型，但没有拿第一阶段的预训练模型有监督地去做第二阶段的Finetuning任务，而是选择了无监督地去做下游任务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
